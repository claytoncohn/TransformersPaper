# TransformersPaper
This repo is created by Clayton Cohn for Vanderbilt University's _DS5899 — Special Topics in Data Science: Transformers_ class paper presentation. Links are provided in-line.

## Overview (Video)

Video Link

## Presentation Matials

Link to slides

Sequence Classification w/ BERT (link to repo)

Tagging w/ BERT (link to repo)

## Question 1

## Question 2

## Code Demonstration

## Resource Links

[BERT Explained](https://medium.com/@samia.khalid/bert-explained-a-complete-guide-with-theory-and-tutorial-3ac9ebc8fa7c)

Illustrated Transformer

Illustrated BERT

BERT Sequence Classification Tutorial

BERT NER Tutorial

[BERT parameters broken down](https://stackoverflow.com/questions/64485777/how-is-the-number-of-parameters-be-calculated-in-bert-model)

## References

<a id="1">[1]</a>
[Mikolov, T., Chen, K., Corrado, G., and Dean, J., “Efficient Estimation of Word Representations in Vector Space”, <i>arXiv e-prints</i>, 2013.](https://arxiv.org/abs/1301.3781)

<a id="1">[1]</a>
[Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. "Glove: Global vectors for word representation." <i>Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</i>. 2014.](https://aclanthology.org/D14-1162.pdf)

<a id="1">[1]</a> 
[Vaswani, A., “Attention Is All You Need”, <i>arXiv e-prints</i>, 2017.](https://arxiv.org/abs/1706.03762)

<a id="1">[1]</a>
[Peters, M. E., “Deep contextualized word representations”, <i>arXiv e-prints</i>, 2018.](https://arxiv.org/abs/1802.05365)

<a id="1">[1]</a> 
[Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. <i>OpenAI blog, 1</i>(8), 9.](https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf)

<a id="1">[1]</a> 
[Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K., “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”, <i>arXiv e-prints</i>, 2018.](https://arxiv.org/abs/1810.04805)

<a id="1">[1]</a> 
[Liu, Y., “RoBERTa: A Robustly Optimized BERT Pretraining Approach”, <i>arXiv e-prints</i>, 2019.](https://arxiv.org/abs/1907.11692)
