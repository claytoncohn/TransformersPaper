# TransformersPaper
This repo is created by Clayton Cohn for Vanderbilt University's _DS5899 — Special Topics in Data Science: Transformers_ class paper presentation. Links are provided in-line.

## Overview (Video)

## Presentation Matials

## Question 1

## Question 2

## Code Demonstration

## Resource Links
Illustrated Transformer
Illustrated BERT
BERT Sequence Classification Tutorial
BERT NER Tutorial

## References
<a id="1">[1]</a> 
[Vaswani, A., “Attention Is All You Need”, <i>arXiv e-prints</i>, 2017.](https://arxiv.org/abs/1706.03762)

<a id="2">[2]</a> 
[Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K., “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”, <i>arXiv e-prints</i>, 2018.](https://arxiv.org/abs/1810.04805)

<a id="3">[3]</a> 
[Liu, Y., “RoBERTa: A Robustly Optimized BERT Pretraining Approach”, <i>arXiv e-prints</i>, 2019.](https://arxiv.org/abs/1907.11692)
